# java并发编程入门
### 基本概念
#####  并发： 同时拥有俩个或者多个线程，如果线程在单核处理器上运行，多个线程将交替的换入或者换出内存，这些线程是同时 "存在" 的，每个线程都处于执行过程中的某个状态，如果运行在多核处理器上，此时程序中的每个线程都将分配到一个处理器核上，因此可以同时运行
##### 高并发： 高并发( High Concurrency )是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求
##### 并发与高并发 的区别：
###### 1、并发：多个线程操作相同的资源，保证线程安全， 合理使用资源
###### 2、高并发： 服务能同时处理很多请求，提高程序性能 
## 并发编程的基础
### CPU多级缓存
##### 为什么需要CPU cache ： 
    cpu的频率太快了，快到主存跟不上，这样在处理时钟周期内，CPU常常需要等待主存，浪费资源。
    所以cache的出现，是为了缓解CPU和内存之间素的的不匹配问题
    (结构 : cpu -> cache -> memory)
    
##### CPU cache有什么意义 ：
    1、时间局限性：如果某个数据被访问， 那么在不久的将来它很可能被再次访问
    2、空间局限性：如果某个数据被访问， 那么与它相邻的数据很快也可能被访问 
    
##### CPU多级缓存 - 缓存一致性 ( MESI )协议
    用于保证多个CPU cache之间缓存共享数据的一致
    
##### CPU多级缓存 - 乱序执行优化
    处理器为条运算速度而做出的为被代码原有顺序的优化
    
### Java内存模型( java Memory Model, JMM )
##### Java内存模型规范: 
    规定了一个线程如何核实看到由其他线程修改后的共享变量的值，以及在必须是如何同步的访问共享变量。
##### 堆：
###### java中的堆是一个运行时 数据区，堆是由垃圾回收来负责的，
    堆的优势是： 可以动态的分配内存大小，生存期也不必事先告诉编译器，因为它是在运行时动态分配内存的，java的垃圾收集器会自动搜索这些不再需要的数据
    堆的缺点是： 由于是在运行时动态分配内存，因此存取速度相对慢一些
##### 栈：
    栈的优势是： 栈的存取速度比堆快一些，仅次于计算机中的寄存器，栈的数据是可以共享的，
    栈的缺点是： 栈的数据大小与生存期必须是确定的，缺乏一些灵活性，
###### 栈中主要存放一些基本类型的变量(boolean、byte、char、short、int、float、double、long) 和对象句柄，
###### java内存模型 
    要求调用栈和本地变量(局部变量)存放在线程栈上，对象存放在堆上。
    方法中的本地变量(局部变量)也是存在线程栈上，即使对象是存在于堆上
    一个对象的成员变量跟随对象一起存在于堆上，（不论这个变量是基本数据类型还是引用数据类型）
    静态成员变量跟随类的定义一起存在于堆上
##### 注意: 
###### 当一个线程访问一个对象， 就可以访问这个对象的成员变量，
###### 当两个线程同时访问一个对象的同一个方法，并且该方法中还调用了该对象的成员变量，那么这俩个线程就会用同时拥有这个对象的成员变量的拷贝，这时候会发生问题

##### Java内存模型 - 同步八种操作
###### 1、lock(锁定) : 作用于主内存的变量， 把一个变量标示为一条线程独占状态
###### 2、unlock(解锁) : 作用于主内存的变量， 把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定
###### 3、read(读取) : 作用于主内存的变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用
###### 4、load(载入) : 作用于工作内存的变量，它把read操作从主内存中得到的变量值放入到工作内存的变量副本中
###### 5、use(使用) : 作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎
###### 6、assign(赋值) : 作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量
###### 7、store(存储) : 作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后额write操作
###### 8、write(写入) : 作用于主内存的变量， 它把store操作从工作内存中一个变量的值传送到主内存的变量中

##### Java内存模型 - 同步规则
###### 1、如果要把一个变量从主内存中赋值到工作内存， 就需要按顺序的执行read和load操作， 如果把变量从工作内存中同步回主内存中，就要按顺序地执行store和write操作。但Java内存模型只要求上述操作必须按顺序执行，而没有保证必须是连续执行
###### 2、不允许read和load、store和write操作之一单独出现
###### 3、不允许一个线程丢弃它的最近assign的操作， 即变量在工作内存中改变了之后必须同步到主内存中
###### 4、不允许一个线程无原因地 (没有发生过任何assign操作) 把数据从工作内存同步回主内存中
###### 5、一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化 (load 或 assign ) 的变量，即就是对一个变量实施use和store操作之前，必须先执行过了一个assign和load'操作
###### 6、一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次， 多次执行lock后， 只有执行相同次数的unlock操作，变量才会被解锁。lock和unlock必须成对出现
###### 7、如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值
###### 8、如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个呗其他线程锁定的变量
###### 9、对一个变量执行unlock操作之前，必须先把此变量同步到主内存中(执行store和write操作) 
### 并发的优势与风险
##### 优势:
###### 速度：
    系统可以同时处理多个请求，响应更快；
    复杂的操作可以分成多个进程同时进行
###### 设计：
    程序设计在某些情况下更简单，也可以有更多的选择
###### 资源引用
    CPU能够在等待IO时做一些其他的事情
    
##### 风险：
###### 安全性:
    多个线程共享数据时可能会产生与期望不相符的结果
###### 活跃性:
    某个操作无法继续进行下去时，就会发生活跃性问题，比如死锁，饥饿等问题
###### 性能:
    线程过多是会使得：CPU频繁切换，调度时间增多；同步机制；消耗过多内存
    
### 并发模拟
##### 1、Postman ： HTTP请求模拟工具
##### 2、Apache Bench (AB) ： Apache 附带的工具， 测试网站性能
    ApacheBench 是 Apache 服务器自带的一个web压力测试工具，简称ab。ab又是一个命令行工具，
    对发起负载的本机要求很低，根据ab命令可以创建很多的并发访问线程，模拟多个访问者同时对
    某一URL地址进行访问，因此可以用来测试目标服务器的负载压力。总的来说ab工具小巧简单，
    上手学习较快，可以提供需要的基本性能指标，但是没有图形化结果，不能监控。
###### AB 参数配置 https://blog.csdn.net/water_tone/article/details/79003059
    ab -n 1000 -c 50 http://localhost:8080/test 
    -n 本次测试总是为多少个
    -c 本次测试的并发数为多少个 
###### 测试结果讲解
    Concurrency Level:      50                                                          // 测试的并发量为50
    Time taken for tests:   0.287 seconds                                               // 测试用的时间
    Complete requests:      1000                                                        // 完成的请求数
    Failed requests:        0                                                           // 失败的请求数
    Total transferred:      136000 bytes                                                // 所有请求的响应数据的长度总和，包括每个Http响应数据的头信息和正文数据的长度，
                                                                                           这里不包括Http请求数据的长度，仅仅展示web服务器流向用户应用层数据的长度
    HTML transferred:       4000 bytes                                                  // 所有请求的响应数据中正文数据的长度总和(总长度-头信息)
    Requests per second:    3481.89 [#/sec] (mean)                                      // 吞吐率 与并发数相关 (Complete requests) / (Time token for tests)
    Time per request:       14.360 [ms] (mean)                                          // 用户平均请求等待时间
    Time per request:       0.287 [ms] (mean, across all concurrent requests)           // 服务器平均请求等待时间
    Transfer rate:          462.44 [Kbytes/sec] received                                // 请求在单位时间内从服务器获取的数据长度  (Total transferred) / (Time taken for tests)
    
    Connection Times (ms)
                  min  mean[+/-sd] median   max
    Connect:        0    0   0.4      0       1
    Processing:     2   14   1.8     14      17
    Waiting:        1    8   3.7      8      16
    Total:          2   14   1.8     14      17
    
    Percentage of the requests served within a certain time (ms)
      50%     14
      66%     14
      75%     15
      80%     15
      90%     15
      95%     16
      98%     16
      99%     16
     100%     17 (longest request)
##### 3、JMeter ： Apache组中开发的压力测试工具
###### 比AB更强大， 用的最多的
##### 4、代码 ： Semaphore、CountDownLatch等 对并发的模拟 
###### CountDownLatch类 向下减的 可以阻塞线程， 当线程满足一定的条件才可以向下执行
    countDownLatch这个类使一个线程等待其他线程各自执行完毕后再执行。
    是通过一个计数器来实现的，计数器的初始值是线程的数量。每当一个线程执行完毕后，计数器的值就-1，
    当计数器的值为0时，表示所有线程都执行完毕，然后在闭锁上等待的线程就可以恢复工作了。
###### Semaphore 信号量  也可以阻塞线程

### 线程安全性
##### 定义：当多个线程访问某个类时，不管运行时环境采用何种调用方式或者这些进程将如何交替执行，并且在主调代码中不需要任何额外的同步或协同，这个类都能表现出正确的行为，那么就称这个类是线程安全的
##### 线程安全性主要体现在三个方面: 
###### 1、原子性: 提供了互斥访问，同一时刻只能有一个线程来对他进行操作
###### 2、可见性: 一个线程对主内存的修改可以及时的被其他线程观察到
###### 3、有序行: 一个线程观察其他线程中的指令执行顺序，由于指令重排序的存在，该观察结果一般杂乱无序

##### 原子性 - Atomic包
###### AtomicXXX : CAS、 Unsafe.compareAndSwapInt
###### CAS原理实现
    public final int getAndAddInt(Object var1, long var2, int var4) {
            int var5;
            do {
                // 从底层取到当前对象变量的值
                var5 = this.getIntVolatile(var1, var2);
                // compareAndSwapInt 判断当前var1对象对应的值var2是否与主存取出的var5的值相等，
                // 如果相等就更新主存当前对象对应的值var5为var5 + var4，并跳出循环 
                // 如果不相等就更新当前线程缓存中（工作内存）主存对象值的拷贝值var2为var5，然后再循环判断
            } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));
    
            return var5;
        }
    
###### ActomicLong、 LongAdder(jdk1.8)
###### LongAdder 优点：
    因为CAS底层实现是在一个while死循环内，不断的尝试修改目标值，直到修改成功，在竞争不激烈的时候，它修改成功的概率很高，如果竞争激烈额话，它修改失败的概率就很高，这个时候性能就会受到影响
    对于普通类型的long、double类型，JVM允许将64位的读操作或者写操作，拆成俩个32位操作
###### LongAdder原理实现：
    是将热点数据分离，比如可以将AtomicLong内部核心数据value，分离成一个数组，每个线程访问时，通过Hash等算法，映射到其中一个数字进行计数，最终的计数结果则为这个数组的求和累加，
    其中热点数据value会被分离成多个单元的shell，每个shell独自维护内部的值，当前对象的值由所有shell累计合成，这样就进行了有效的分离并提高了并行度，
    这样一来LongAdder就在AtomicLong的基础上将单点的压力分担在各个节点上，
    在低并发的时候通过对底层base的更新达到和AtomicLong一样的效果，
    在高并发的时候通过分担节点来提高性能
    
    缺点: 
        在统计的时候，如果有并发更新，统计的结果会有一些误差
        
###### AtomicReference、AtomicReferenceFieldUpdater 
###### AtomicStampReference : CAS 的 ABA 问题
    什么是CAS的ABA问题？
        ABA问题是指在CAS操作的时候其他线程将变量的值A改成了B，然后有改成了A，
        本线程在CAS操作的时候发现当前底层变量的值和工作内存中的值相等，进而进行了更新操作，
        这个时候实际上值已经被其他线程修改过， 这与CAS本身的设计思想是不相符的    
    ABA的解决思路：
        每次变量更新的时候将变量的版本号加1，1A -> 2B -> 3A 
###### AtomicStameReference源码
    public boolean weakCompareAndSet(V   expectedReference,
                                         V   newReference,
                                         // 期望的版本号
                                         int expectedStamp,
                                         // 新的版本号
                                         int newStamp) {
            return compareAndSet(expectedReference, newReference,
                                 expectedStamp, newStamp);
        }
        
###### AtomicLongArray
    和 AtomicLong方法基本类似， 只是会多加了一个索引值参数
###### 实例 AtomicBoolean 中的compareAndSet(boolean expect, boolean update) 

##### 原子性 - 锁
    原子性提供了互斥访问，同一时刻只能有一个线程来对他进行操作
    能保证统一时刻只有一个线程来对它进行操作的处理原子性，还有锁
###### synchronized : 依赖JVM
    它是Java的关键字，主要是依赖JVM去实现锁，因此在这个关键字作用对象的作用范围内，都是统一时刻只能有一个线程进行操作的
###### Lock : (代码层面的锁) 依赖特殊的CPU指令， 代码实现， ReentrantLock
###### 原子性 - synchronized
    1、修饰代码块 : 大括号括起来的代码，作用于调用的对象
    2、修饰方法 : 整个方法，作用于调用的对象
    3、修饰静态方法 : 整个静态方法， 作用于所有对象
    4、修饰类 : 括号括起来的部分，作用于所有对象  
###### 子类在继承父类的时候，继承到的方法是不包含synchronized关键字的
    因为synchronized是方法声明的一部分， 如果子类也想要synchronized需要显示的声明
###### 原子性 - 对比
     synchronized : 不可中断锁， 适合竞争不激烈，可读性好
     Lock : 可中断锁，多样化同步，竞争激烈是能维持常态
     Atomic : 竞争激烈是能维持常态， 比Lock性能好； 只能同步一个值
##### 可见性
###### 导致共享变量在线程间不可见的原因
    1、线程交叉执行
    2、重排序结合成成交叉执行
    3、共享变量跟新好的值没有在工作内存与主内存间及时更新
    
###### 可见性 - synchronized
    JVM关于Synchronized的俩条规定
        1、线程解锁前，必须把共享变量的最新值刷新到主内存
        2、线程加锁时、将清空工作内存中共享变量的值，从而使用共享变量是需要从主内存中重新读取罪行的值(注意， 加锁与解锁是同一把锁)
###### 可见性 - volatile  (不具有原子性)
    通过加入内存屏障和禁止重排序优化来实现
        1、对volatile变量写操作时，会在写操作后加入一条store屏障指，将本地内存中的共享变量值刷新到主内存
        2、对volatile变量读操作时，会在读操作前加入一条load屏障指令，从主内存中读取共享变量
###### 可见性 - volatile使用  (适合作为状态标记量， double-check单例模式中)
###### 1、对变量的写操作不依赖当前值
###### 2、该变量没有包含对其他变量不必要的式子中 
        volatile boolean inited = fales;
        
        // 线程 1:
        context = loadContext();
        inited = true;
        
        // 线程 2:
        while( !inited ){
        sleep();
        }
        doSomethingWithConfig(context); 
        
##### 有序性
###### 1、Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多项城并发执行的正确性
###### 2、可以通过 volatile、synchronized、Lock来保证指令的有序性
###### 有序性 - happens-before原则
###### 1、程序次序规则 : 一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后边的操作
    程序看起来执行顺序是按照代码顺序执行的，但是虚拟机可能会对程序执行指令重排序来进行优化，
    虽然执行了重排序，但是程序执行的结合和按代码顺序执行的结果是一样的
    它只会对不存在数据依赖性的程序执行指令重排序
###### 2、锁定原则 : 一个unLock操作先行发生于后面对同一个锁的lock操作
    无论是单线程操作还是多线程操作，同一个锁如果处于被锁定状态，那么必须先对锁进行释放操作，
###### 3、volatile变量规则 : 对一个变量的写操作先行发生于后面对这个变量的读操作
    如果一个线程先去写一个变量，然后一个线程去读取，那么写操作会先行发生于读操作
###### 4、传递规则 : 如果操作A先行发生于操作B，而操作B有先行发生于操作C，则可以得到操作A先行发生于操作C
###### 5、线程启动规则 : Thread对象的start()方法先行发生于此线程的每一个动作
###### 6、线程中断规则 : 对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生
###### 7、线程终结规则 : 线程中所有的操作都先行发生于线程的终止检测， 我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行
###### 8、对象检测原则 : 一个对象的初始化完成先行发生于他的finalize()方法的开始


### 发布对象
##### 发布对象 ：是一个对象能够被当前范围之外的代码所使用
##### 对象溢出 ：一种错误的发布， 当一个对象还没有构造完成是，就使他被其他线程所见
#### 安全发布对象
##### 1、在静态初始化函数中初始化一个对象引用
##### 2、将对象的引用保存到volatile类型域或者AtomicReference对象中
##### 3、将对象的应用保存到某个正确构造对象的final类型域中
##### 4、将对象的应用保存到一个由锁保护的域中 

### 不可变对象 (安全发布对象)
#### 不可变对象满足的条件 (参照String类)
##### 1、对象创建以后其状态就不能修改
##### 2、对象所有的域都是final类型
##### 3、对象是正确创建的 (在对象创建期间， this引用没有溢出) 
#### final关键字 ： 类、方法、变量
##### 修饰类 ：不能被继承 (Integer Long String ...)
    final 类中的所有成员方法，都会被隐式的被final修饰
##### 修饰方法： 1、锁定方法不被继承类修改； 2、效率
##### 修饰变量：
    基本数据类型变量：数值一旦初始化之后就不能再修改了
        传入方法的基本类型变量如果被final修饰，那么它也是不能被修改的
    引用类型变量：则在对其初始化之后则不能让它再指向另外一个对象
#### Collections.unmodifiableXXX: Collection、List、Set、Map...
##### java中的不允许修改的工具类， 只需要将不想被修改的Collection、List、Set、Map... 作为参数放入到这个方法中，就能得到
##### Guava: Google的Guava包中新增的ImmutableXXX : Collection、List、Set、Map...

#### 线程封闭
    把对象封装到一个线程里，只有一个线程能看到这个对象
##### 1、Ad-hoc线程封闭 ：程序控制实现，最糟糕，忽略
    完全靠实现者控制的线程封闭
##### 2、堆栈封闭 ：局部变量，无并发问题
    局部变量，多个线程访问同一个局部变量的时候，每个线程会将局部变量拷贝到自己的工作内存中
##### 3、ThreadLocal 线程封闭 ： 特别好的封闭方法
    数据库连接对应JDBC的Connection对象，
    Connection在实现时并没有对线程安全做处理， Java规范中也没有要求Connection是线程安全的
    实际上在服务器应用程序中，线程从连接池获取一个Connection对象，使用完成以后再将Connection放回到连接池中
    由于大多数请求都是由单线程采用同步的方式来处理的，并且在Connection返回之前，连接池不会将它分配给其他线程
    因此这种连接管理模式在处理请求时隐含的将Connection对象封闭在线程里边，也就实现了线程安全
    
#### 线程不安全类与写法
##### StringBuilder -> StringBuffer
##### SimpleDateFormat -> JodaTime
##### ArrayList、 HashSet、 HashMap 等 Collections
##### 先检查再执行 ： if(condition(a)) { handle(a); }

#### 线程安全 - 同步容器
##### 同步容器不一定是线程安全的
##### ArrayList -> Vector, Stack
    Vector：
        Vector实现了List接口，实际上就是一个数组，Vector中的方法都是被synchronized修饰的
    Stack ：
        继承于Vector类， 也是用synchronized修饰的，就是数据结构中的栈(先进后出)
##### HashMap -> HashTable(key, value不能为null)
    HashTable内部进行了同步代码
##### Collections.synchronizedXXX(List、Set、Map)
    Collections 是一个工具类， 提供了大量的方法，包括同步容器的创建
##### 针对集合的删除操作：
    在遍历的时候不要进行删除操作，对要删除的数据进行标记， 等待遍历结束以后进行统一的删除处理
#### 线程安全 - 并发容器 J.U.C （Java.Util.Concurrent）
##### ArrayList -> CopyOnWriteArrayList
    CopyOnWriteArrayList ：写操作的时候复制，
        当有新元素添加到CopyOnWriteList的时候，它从原有的数组里边拷贝一份，然后在新的数组上进行拷贝操作，写完之后再将原来的数组指向新的数组
        整个add操作都是在锁的基础上操作的
        缺点：
            1、进行写操作的时候需要copy数组， 比较消耗内存
            2、不能用于实时读的场景，更适合读多写少的场景
###### CopyOnWriteArrayList的设计思想是：
    1、读写分离，让读和写分开
    2、最终一致性
    3、使用时另外开辟空间， 通过这种方式来解决并发冲突
    4、读的时候不需要加锁， 写的时候需要加锁，防止多个线程复制出多个副本
##### HashSet、TreeSet -> CopyOnWriteArraySet、ConcurrentSkipListSet
    CopyOnWriteArraySet、ConcurrentSkipListSet （不允许有null）
        单独的add操作是线程安全的
        如果使用containsAll()、removeAll()、retainAll()的时候是需要加锁的，
        因为它只能保证每一次add是原子操作的，不能保证多个add的时候不会被其他线程打断
        
##### HashMap、TreeMap -> ConcurrentHashMap、ConcurrentSkipListMap
    ConcurrentHashMap、ConcurrentSkipListMap (不允许有null值)
    ConcurrentHash是HashMap的多线程版本，针对读操作做了很多优化，具有很高的高并发性能
    ConcurrentSkipListMap是TreeMap的多线程版本，内部是使用SkipList这种跳表的结构来实现的
        4个线程 1.6w数据量的情况下，ConcurrentHashMap的存取速度是ConcurrentSkipListMap的4倍
    在非多线程的情况下尽量使用HashMap和TreeMap        
    在并发性相对较低的并行程序，也可以使用Collections类
        synchronizedSortedMap()， 也能提高程序的效率
###### ConcurrentSkipListMap有几个是ConcurrentHashMap不能比的有点
    1、ConcurrentSkipListMap的key是有序的
    2、ConcurrentSkipListMap支持更高的并发，它存取时间和线程数几乎是没有关系的，
        也就是说在数据量一定的情况下，线程数越多ConcurrentSkipListMap的性能越好
### 安全共享对象策略 - 总结
#### 1、线程限制 ： 一个被线程限制的对象，由线程独占， 并且只能被占有它的线程修改       
#### 2、共享只读 ： 一个共享只读的对象，在没有额外同步的情况下，可以被多个线程并发访问，但是任何线程都不能修改它
#### 3、线程安全对象 ： 一个线程安全的对象或者容器，在内部通过同步机制来保证线程安全，所以其他线程无需额外的同步就可以通过公共接口随意访问它
#### 4、被守护对象 ： 被守护对象只能通过获取特定的锁来访问

### J.U.C 之 AQS
#### AbstractQueuedSynchronizer - AQS(同步器)
    java5之后引入了J.U.C，大大提高了Java并发的性能，AQS可以说是J.U.C的核心
    提供了Fist IN First OUT 
    底层使用的是双向列表
##### 1、使用Node实现FIFO队列，可以用于构建锁或者其他同步装置的基础框架
##### 2、利用了一个int类型表示状态
##### 3、使用方法是继承
##### 4、子类通过继承并通过实现他的方法管理其状态{ acquire 和 release }的方法操纵状态
##### 5、可以同时实现排他锁和共享锁模式 (独占、共享)
###### 在一个使用者的角度，AQS的功能主要分为两类：独占功能和共享功能
    AQS所有的子类中，要么实现并使用了他独占功能的API， 要么使用了共享锁的功能，不会使用两套API
##### AQS具体实现的大致思路：
###### 1、首先AQS内部维护了一个CLH队列来管理锁，线程会首先尝试获取锁，如果失败，将当前线程以及等待等信息包成一个node节点，加入到同步队列SyncQueue里
###### 2、接着会不断尝试的获取锁，条件是当前节点为Head的直接后继才会尝试获取锁，如果失败就会阻塞自己，直到自己被唤醒
###### 3、当持有锁的线程释放锁的时候会唤醒队列中的后继线程 


#### AQS 同步组件
##### CountDownLatch
    它是一个闭锁，通过一个计数来保证线程是否需要一直阻塞
###### CountDownLatch是一个同步辅助类，可以阻塞当前线程，用了一个给定的计数器进行初始化，该计数器的操作是原子操作(同时只有一个线程可以操作该计数器)，调用awaiting()的线程会一直处于阻塞状态，直到其他线程调用countDown()方法使当前计数器变成0，每次调用countDown()的时候,计数器的值会减1，当计数器的值减到0的时候，所有因调用await()而处于等待状态的线程继续执行，这里的计数器不能被重置
###### 使用场景：
    在某些业务场景中，程序执行需要等到某个业务场景完成后，才能继续执行后续操作，典型的应用比如：
        并行计算：将一个大的任务拆分成多个子任务，并行执行，等所有子任务都结束后在统计父任务的结果
    案例代码中并发模拟的时候用的就是CountDownLatch，保证所有的线程都执行完之后才进行统计求和的结果
##### Semaphore （信号量）
    他能控制同一时间并发线程的数目
###### 使用场景：
    常用于仅能提供有限访问资源， 比如项目中的数据库，数据库的连接数的限制
##### CyclicBarrier
###### 是一个同步辅助类，允许一组线程相互等待，直到到达某个公共的屏障点(CommonBarrierPront),通过它可以完成多个线程之间相互等待，只有当每个线程准备就绪后才能继续往下执行后续的操作，和CountDownLatch类似，都是通过计数器来实现的，
    区别:
        CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法进行循环使用
        CountDownLatch用于实现一个或多个线程等待其他线程完成某项操作之后继续执行（描述的是一个或多个线程等待其他线程的关系），
            而CyclicBarrier主要是实现了多个线程相互等待，直到所有线程都满足了条件之后才能继续执行后续的操作（描述的是各个线程内部相互等待的关系）
###### 所以CyclicBarrier能处理更复杂的业务场景
##### ReentrantLock 与 锁
###### Java主要分两类锁： 一种是Synchronized修饰的锁， 另外一种就是J.U.C里边提供的锁
###### ReentrantLock就是 J.U.C 里边提供的锁 核心也是 lock 和 unlock
###### 特点
    1、可重入锁
        再进入锁，和Synchronized类似，都是同一个线程进入一次锁的计数器，就自增1，所以要等锁的计数器下降为0时才能释放锁
    2、锁的实现
        synchronized是依赖JVM实现的， ReentrantLock是JDK实现的
        区别：
            前者相当于操作系统来控制实现，不容易查看源码
            后者相当于用户用程序控制实现，可以查看源码
    3、性能的区别：
        Synchronized 和 ReentrantLock的性能现在是差不多的，在两者都可以用的情况下官方更推荐Synchronize，因为写法更简单
        其实Synchronized的优化感觉就是借鉴了ReentrantLock的CAS技术，都是在用户态就把加锁问题解决，避免进入内核态的线程阻塞
    4、功能区别：
        1）、便利性：Synchronized更方便使用，由编译器去保证锁的加锁和释放的，而ReentreantLock是由程序手动加锁和释放的
        2）、锁的细粒度和灵活度：ReentrantLock会优于Synchronize
###### 公平锁：先等待的线程先获得锁
###### ReentrantLock独有的功能
    1、可指定是公平锁还是非公平锁，而Synchronized只能是非公平锁
    2、提供了一个Condition类，可以分组唤醒需要唤醒的线程
    3、提供能够中断等待锁的线程的机制， lock.lockInterruptibly()
###### ReentrantReadWriteLock， 内部包含了读写锁的概念，是一种悲观锁的体现
###### StampedLock 对吞吐量有巨大的提升
    StampedLock 它控制锁有三种模式
        写、读、乐观读
    一个StampedLock状态是由版本、模式俩个部分组成，锁获取方法返回是一个数字作为票据(Stamp)，它用相应的锁状态来表示和控制锁访问
        0 表示没有写锁被首先访问，在读锁上分为 悲观锁 和 乐观锁
    所谓乐观读，就是读的操作很多，写的操作很少，我们可以乐观的认为写入与读取发生的几率很少，因此不悲观的使用完全锁定
    
##### 线程选择总结：
###### 1、当只有少量的线程的时候，Synchronized是一个很少的通用的锁实现
###### 2、线程不少，但是线程增长的趋势我们是可以预估的， 这个时候ReentrantLock是一个很好的实现
##### Condition

##### FutureTask
###### 这个组件是 J.U.C 里的，但是不AQS的子类，但是这个类对线程结果的处理很适合我们去学习
###### Thread 和 Runnable 的缺点是在线程执行结束后，不能够获取线程的结果。在Java1.5以后就引入了 Callable 和 Future，可以在任务执行完成之后得到任务执行的结果
###### Callable 和 Runnable 接口对比
###### Future 接口 ：可以得到别的线程方法的返回值
###### FutureTask 类 :相当于之前把使用Callable和Runnable都整合了，非常的方便

##### Frok/Join 框架
###### 是Java7 提供的用于并行执行任务的框架，它是一个把大任务分割成若干个小任务，最终汇总每个小任务结果，得到大任务结果的框架。
###### 主要采用的是工作窃取算法： 某个线程从其他队列里窃取任务来执行
###### 对于Frok/Join框架而言，当一个任务正在等待它使用Join操作创建的子任务结束时，执行这个任务的工作线程查找其他未被执行的任务并开始执行，通过这种方式线程充分利用他们的停留时间，来提高应用程序的性能
###### Frok/Join框架的局限性:
    1、任务只能使用Frok和Join操作来作为同步机制，如果使用了其他同步机制，那他们在同步操作时，工作线程就不能执行其他任务了，（如果一个任务执行了睡眠，那么正在执行这个任务的线程就不会执行其他任务了）
    2、所拆分的任务不能去执行IO操作
    3、任务不能抛出检查异常，必须通过必要的代码阻止他们
##### BlockingQueue (阻塞队列) 
###### BlockingQueue 不仅实现了一个完整队列所具有的基本功能， 同时在多线程环境下，他还自动管理了多线程间的自动等待， 唤醒功能，从而使得开发人员可以忽略这些细节，去关注一些高级的功能
###### 主要用在生产者消费者场景，主要实现类：
###### ArrayBlockQueue: 先进先出,大小一旦指定就不能变
###### DelayQueue：阻塞的是内部元素， DelayQueue内部元素必须实现一个接口----Delayd --- 继承了Runnable，主要是DelayQueue中的元素需要排序，一般都是按照元素过期时间的优先级进行排序
    应用场景：定时关闭连接，缓存对象、超时处理管理
###### LinkedBlockingQueue 内部实现是一个链表，其他和ArrayBlockQueue类似
###### PriorityBlockingQueue 有排序规则， 有边界， 对象要实现Comparable接口， 可以用迭代器按照优先级顺序进行迭代
###### SynchronizeQueue 同步队列， 无界非缓存队列， 放入元素只有等待取走之后才能放入
### J.U.C 之 线程池
##### new Thread 弊端：
###### 1、每次 new Thread 新建对象，性能差
###### 2、线程缺乏统一管理， 可能无限制的新建线程，相互竞争，有可能占用过多系统资源导致死机或OOM
###### 3、缺少更多功能，如更多执行、定期执行，线程中断
##### 线程池的好处：
###### 1、重用存在的线程，减少对象创建、消亡的开销，性能佳
###### 2、可有效控制最大并发线程数，提高系统资源利用率，同时可以避免过多资源竞争，避免阻塞
###### 3、提供定时执行、定期执行、单线程、并发数控制等功能
#### 线程池 - ThreadPoolExecutor
##### 初始化参数
###### 1、corePoolSize ： 核心线程数量
     如果运行的线程数小于corePoolSize的时候，直接创建新线程来处理任务，即使线程池中的其他线程是空闲的，
     如果线程池中线程的数量大于等于corePoolSize且小于maximumPoolSize，则只有当workQueue满的时候才创建新的线程去处理任务
     如果我们设置的corePoolSize和maximumPoolSize相同的话，那么创建的线程池大小是固定的，这个时候如果workQueue没满的时候，就将新的任务放到workQueue里，等待有线程空闲的时候再去处理这个任务
     如果运行的线程数量大于maximumPoolSize的时候，并且这个时候workQueue也已经满了，这个时候需要决策参数来指定策略
###### 2、maximumPoolSize ： 线程最大线程数
#####(重要) 3、workQueue ： 阻塞队列，存储等待执行的任务，很重要，会对线程运行过程产生重大影响
    它是保存等待执行任务的一个阻塞队列，当我们提交一个新的任务到线程池以后，线程池会根据当前正在执行的线程的数量来决定该任务的处理方式
    处理方式有三种：
        SynchronousQueue ---- 没有容量，是无缓冲等待队列
        LinkedBlockingQueue ---- 无界缓存等待队列 
        ArrayBlockingQueue ---- 有界缓存等待队列 
###### SynchronousQueue没有容量，是无缓冲等待队列，是一个不存储元素的阻塞队列，会直接将任务交给消费者，必须等队列中的添加元素被消费后才能继续添加新的元素。拥有公平（FIFO）和非公平(LIFO)策略，非公平侧罗会导致一些数据永远无法被消费的情况？使用SynchronousQueue阻塞队列一般要求maximumPoolSizes为无界，避免线程拒绝执行操作。
###### LinkedBlockingQueue是一个无界缓存等待队列。当前执行的线程数量达到corePoolSize的数量时，剩余的元素会在阻塞队列里等待。（所以在使用此阻塞队列时maximumPoolSizes就相当于无效了），每个线程完全独立于其他线程。生产者和消费者使用独立的锁来控制数据的同步，即在高并发的情况下可以并行操作队列中的数据。
###### ArrayBlockingQueue是一个有界缓存等待队列，可以指定缓存队列的大小，当正在执行的线程数等于corePoolSize时，多余的元素缓存在ArrayBlockingQueue队列中等待有空闲的线程时继续执行，当ArrayBlockingQueue已满时，加入ArrayBlockingQueue失败，会开启新的线程去执行，当线程数已经达到最大的maximumPoolSizes时，再有新的元素尝试加入ArrayBlockingQueue时会报错。
###### keepAliveTime : 线程没有任务执行时最多保存多久时间终止（线程池维护线程所允许的空闲时间）
    当线程池中的线程数量大于corePoolSize时，如果没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，知道等待的时间超过了keepAliveTime
###### unit ：keepAliveTime的时间单位
###### threadFactory ：线程工厂， 用来创建线程
    默认会有一个默认的创建线程的工厂，使用默认的工厂来创建线程的时候会使新创建的线程具有相同的优先级，并且是非守护的线程，
###### rejectHandler : 当拒绝处理任务时的策略
    如果workQueue这个阻塞队列满了，并且没有空闲的线程时，这个时候继续提交任务，我们就需要采取一种策略来处理任务，
    线程池提供了四种策略：
        1、直接抛出异常 （默认策略）
        2、用调用者所在的线程来执行任务
        3、丢弃队列中最靠前的任务，并执行当前任务，（相当于好久之前的任务不要了，然后执行当前的任务）
        4、直接丢弃这个任务
    
##### 线程池参数设置参考
###### 1、要想使线程池处理任务和吞吐量达到一个相适的范围，又想使我们调度相对简单，并且还能尽可能的降低线程池对资源的消耗，我们就需要合理的设置corePoolSize和maximumPoolSize的数量
###### 2、如果想降低系统资源的消耗，包括CPU的使用率、操作系统资源的消耗、上下文环境切换的开销等 ---------------- 可以设置一个较大的队列容量，和较小的线程池容量，这样会降低线程处理任务的吞吐量
###### 3、如果我们提交的任务经常被阻塞 ------------- 我们可以考虑调用设置线程最大数方法，重新设置线程池容量，
###### 4、如果我们队列容量设置较小，通常需要把线程池容量设置大一点，这样能提高CPU使用率，
###### 5、如果线程池容量设置过大，这样在提交的任务过多的情况下，那么线程之间的调度就是一个要考虑的问题，这样反而有可能会降低处理任务的吞吐量

##### 所以在任务提交的时候，判断的顺序主要为3个：
###### 1、看corePoolSize  小于直接调用， 大于将任务放到workQueue
###### 2、看workQueue 
###### 3、看maximumPoolSize

##### 线程池实例的几种工单状态：
###### 1、running
    在running状态时，它能接受新提交的任务，也能处理在阻塞队列中的任务
###### 2、stop
    线程池不能接受新的任务，也不能处理阻塞队列中的任务，如果线程池处于shutdown状态的时候调用shutdownNow()方法的时候，线程池状态就会变成stop
###### 3、shutdown
    关闭状态，如果当前实例处于shutdown状态，它就不能接受新提交的任务，但是可以继续处理阻塞队列中已经保存的任务
###### 4、tidying
    如果所有的任务已经终止了，有效线程数为0，之后调用terminated() 方法会进入terminated状态
###### 5、terminated
    默认的terminated什么都没有做，只是在调用了terminated()方法之后进入该状态
##### execute() : 提交任务， 交给线程池执行
##### submit() : 提交任务， 能够返回执行结果， execute + Future （如果需要返回结果的时候调用）
##### shutdown() : 关闭线程池， 等待任务都执行完(等待阻塞队列中的任务)
##### shutdownNow() : 关闭线程池，不等待任务执行完(不等待阻塞队列中的任务)
###### -------- 用于监控的方法
##### getTaskCount() : 线程池已执行和未执行的任务总数
##### getCompletedTaskCount() : 已完成的任务数量
##### getPoolSize() : 线程池当前的线程数量
##### getActiveCount() : 当前线程池中正在执行任务的线程数量
###### -------- J.U.C 提供了额外的方法  (Executors 工具类)
##### Executors.newCachedThreadPool 
    可以创建一个可缓存的线程池，如果线程池的长度超过了处理的需要可以灵活回收空闲的线程，如果没有可以回收的，那么就新建线程
##### Executors.newFixedThreadPool
    它创建的是一个定长的线程池，可以控制线程的最大并发数，超出的线程会在阻塞队列中等待
##### Executors.newScheduledThreadPool
    它创建的也是一个定长的线程池，它支持定时、以及周期性的任务执行
##### Executors.newSingleThreadExecutor
    它创建的是一个单线程化的线程池，它只用唯一的工作线程来执行任务，保证所有的任务都按指定的顺序执行，这个指定顺序可以按照先进先出，优先级等等来去执行
    
#### 线程池 - 合理配置
##### CPU密集型任务，就需要尽量压榨CPU，参数值可以设为 NCPU + 1 (CPU数量加1)
##### IO密集型任务，参考值可以设置为2 * NCPU

### 多线程并发拓展
#### 死锁
##### 所谓的死锁，是指俩个或两个以上的进程在执行过程中，因单个资源造成的一种互相等待的现象，没有外力作用它们将无法推进下去，此时我们就称系统处于死锁状态，或者产生了死锁，这些互相等待的线程称为死锁进程。
#### 死锁发生的必要条件：
##### 1、互斥条件
##### 2、请求和保持条件
##### 3、不剥夺条件
##### 4、环路等待条件
#### 多线程并发最佳实践
##### 1、使用本地变量
    尽量使用本地变量(局部变量)，而不是创建一个类或者使用实例变量
##### 2、使用不可变类
    不可变类比如String、Integer类等，一旦创建就不可改变了，不可变类可以降低代码中需要的同步数量
##### 3、最小化锁的作用域范围 : S = 1/(1 - a + a/n) 阿姆达尔定律（an达尔定律）
    a：并行计算部分锁占的比例
    n：并行处理的节点个数
    s：加速比
##### 4、使用线程池的Executor， 而不是直接new Thread执行
##### 5、宁可使用同步也不要使用线程的wait和notify
    java1.5之后增加了很多同步工具：CountDownLatch、Semaphore、CyclicBarrier
    应该优先考试使用这些同步工具，而不是考虑使用线程的wait和notify方法
    通过BlockingQueue实现生产和消费的设计化比使用线程的wait和notify要好的多，也可以使用CountDownLatch实现多个线程的等待等。
##### 6、使用BlockingQueue实现生产 - 消费模式
##### 7、使用并发集合而不是加了锁的同步集合
    Java提供了ConcurrentHashMap，CopyOnWriteArrayList、CopyOnWriteArraySet、以及BlockingQueue
##### 8、使用Semaphore创建有界的访问
    为了建立可靠有序的系统，对数据库、文件系统和Socket等资源，需要作有序的访问，可以使用Semaphore控制同时访问指定资源的线程数
##### 9、宁可使用同步代码块，也不使用同步的方法 (Synchronize)
##### 10、避免使用静态变量
    如果必须使用静态变量，优先让它成为final变量    
    
#### Spring与线程安全 
##### 1、Spring bean ： singleton(默认的scope)、prototype(每次注入都有一个实例产生)
##### 2、无状态对象 
#### HashMap 与 ConcurrentHashMap
    HashMap 允许key、value为null， ConcurrentHashMap 不允许
    
## 高并发处理思路与手段
### 扩容
#### 1、垂直扩展(纵向扩展) ：提高系统部件能力
#### 2、水平扩展(横向扩展) ：增加更多系统成员来实现
    增加服务器
#### 扩容 - 数据库
##### 1、读操作扩展 ； memcache、redis、CDN等缓存
##### 2、写操作扩展 ： Cassandra、Hbase 等

## 高并发之缓存
### 缓存
    现在的互联网应用、网站或者App的流程大致如下流程：
    
                                   服务A
    浏览器 --->    网络转发  --->   服务B ——>    数据库   
    (APP)    <--- (Linux)等   <--- 服务C     <——(存储)
                                   服务D
                                   
       ①             ②              ③             ④
##### 缓存的使用可以出现在 1 - 4 各个环节中

#### 缓存特征
##### 1、命中率 ：命中数 / (命中数 + 没有命中数)
##### 2、最大元素(空间)
##### 3、清空策略 ：FIFO，LFU，LRU，过期时间，随机等
    FIFO：先进先出策略
        最先进入缓存的数据，在缓存不够的时候，优先删除最先进入缓存的数据， 这类策略用于对实时性要求比较高的环境
    LFU：Least Frequently Used(最近最少使用算法) 最少使用策略
        无论是否过期，根据元素被使用的次数来判断是否被清除，在保证高频数据有效的环境较适合
    LRU：The Least Recently Used(最近最久未使用算法) 最近最少使用策略
        无论是否过期，根据元素最后一次被使用的时间戳，清除最远使用时间戳的元素，主要比较元素最近一次被get使用时间，在热点数据使用场景下较适用
#### 缓存命中率影响因素
##### 1、业务场景和业务需求
    缓存通常适合读多写少的场景
##### 2、缓存的设计(粒度和策略)
    缓存的粒度越小，命中率越高，
##### 3、缓存的容量和基础设施
    
#### 缓存分类和应用场景
##### 1、本地缓存 : 编程实现 (成员变量、局部变量、静态变量)、Guava Cache
##### 2、分布式缓存 ： Memcache、Redis

#### 缓存 - Guava Cache
    清空策略为LRU The Least Recently Used(最近最久未使用算法) 最近最少使用策略
#### 缓存 - Memcache 
    应用比较广的一个开源分布式缓存之一
    slab_class[n] -> slab[n] -> page[n] -> chunk[n]
    
    Memcache的内存分配 chunk里边总会有内存浪费
    Memcache的LRU算法不是针对全局的，而是针对slab的
    
##### Memcache 的限制和它的特性
###### 1、可以保存的item的数量是没有限制的，只要内存足够
###### 2、Memcache单进程在32位系统中，最大可以使用的内存为2G，64位机没有现存
###### 3、Memcache中的key最大为250个字节，超过这个长度是无法存储的
###### 4、单个的item最大数据是1M，超过1M也是无法存储的
###### 5、Memcache的服务器端是不安全的
    比如我们已经知道了某个Memcache节点，我们直接talent过去之后，可以通过flash all命令， 让已经存在的键值对立即失效
###### 6、Memcache他不能遍历他里边所有存储的item
###### 7、Memcache的高性能来源于两个阶段
    第一个阶段在客户端，客户端根据key值算出一个节点，
    第二阶段是在服务端，通过有个内部的hash算法寻找真正的item,并返回给客户端
    从时间的角度看， Memcache是一个非阻塞的基于事件的服务器程序，

#### 缓存 - Redis
##### Redis 是一个远程的内存数据库，菲关系型数据库，性能强劲，具有复制特性，以及解决问题而生的独一无二的数据类型，他能够存储键值队与5种不同类型的值之间的映射，可以将存储在内存的键值队持久化到硬盘，可以使用复制特性来扩展读性能，还可以添加分片来扩展写性能
##### Redis 特点
###### 1、Redis支持数据持久化，可以将内存中的数据保存在磁盘里，重启的时候可以再次加载重新使用，
###### 2、Redis不仅支持简单的 key - value 键值队存储，同时它还提供了List Set Hash Sorted-set
###### 3、Redis支持数据的备份， 主从数据备份
##### Redis的优势
###### 1、性能高，读的速度能到 11w/s, 写的速度能到 81000/s 
###### 2、丰富的数据类型：string hash list set sorted-set
###### 3、redis的所有操作都是原子性的，同时redis还支持对几个操作合并后的原子性执行，
##### Redis的场景
###### 1、取最新N个数据的操作，排汗榜例子的应用
###### 2、精准设计过期时间的应用， 计数器的应用
###### 3、唯一性检查的操作
###### 4、实时的消息系统，队列系统，缓存

#### 高并发场景下缓存常见问题
##### 1、缓存一致性
###### 当数据实效要求很高的时候，我们就需要保证缓存中的数据与数据库中的数据保持一致，而且需要保证缓存节点和副本中的数据也保持一致，不能出现差异现象，这就比较依赖缓存的过期和更新策略，一般会在数据发生更改的时候主动，更新缓存中的数据或者移除对应的缓存，这个时候就可能出现缓存一致性的问题
    更新数据库成功 -----> 更新缓存失败   -----> 数据不一致
    更新缓存成功   -----> 更新数据库失败 -----> 数据不一致
    更新数据库成功 -----> 淘汰缓存失败   -----> 数据不一致
    淘汰缓存成功   -----> 更新数据库失败 -----> 查询缓存miss
##### 2、缓存并发问题
###### 缓存过期后会尝试从后端数据库获取数据，看是合理，但是在高并发情况下，大量请求并发的从数据库获取数据，后端数据库受到了极大的冲击，可能会出现雪崩的现象
###### 如何避免？ 缓存失效设置随机值； 同过锁的机制，在缓存更新或者过期的情况下，先尝试获取到锁，当更新或者从数据库获取完成后，再释放锁，其他请求只需要牺牲一定的等待时间，就可以从缓存中继续获取数据

##### 3、缓存穿透问题
###### 在高并发场景下，如果某一个key被高并发的访问，但是没有命中， 处于对容错性的考虑，会尝试从后端去获取，从而导致了大量的请求到达了数据库，而当该key对应的数据本身为空的情况下，这就导致数据库并发的执行了很多不必要的查询操作，从而导致了巨大的冲击和压力
###### 可以通过以下方式避免缓存穿透问题
    1、缓存空对象， 对查询结果为空的对象也进行缓存，
        如果集合的话，可以缓存一个空的集合，但不是空null，
        如果是缓存单个对象， 可以通过字段标示来区分
       这样避免请求穿透到后端数据库，同时要保证缓存数据的实效性，这种方式成本不高，比较适合命中不高，可能被频繁更新的数据
    2、单独过滤处理，对所有可能对应数据为空的key，进行统一的存放，并在请求前做拦截，这样避免请求穿透到后端数据库
       这种方式实现起来相对复杂，比较适合命中不高，但是更新不频繁的数据
    
##### 4、缓存雪崩现象
###### 缓存颠簸问题(缓存抖动)：是一种比雪崩更轻微的故障，但是也会在一段时间内对系统造成冲击和性能影响，一般是由于缓存节点故障导致，推荐的处理方法是：通过一致性Hash算法来解决
###### 缓存雪崩就是指由于缓存的原因导致大量的请求到达后端数据库，从而导致数据库崩溃，整个系统崩溃,

## 高并发之消息队列
### 消息队列
##### 消息队列已经成为企业内部系统通信的核心手段
#### 消息队列的简单场景
    开始流程A -----> 发送消息A1到消息队列 -----> 消息A1被处理 -----> 处理消息A1(处理流程A)
    例如：商品下单成功之后会给客户发送短信，如果不引用消息队列的情况下，需要同步的调用短息接口，如果这个时候短信接口出现了异常，或者同时发送的短信太多，会导致系统出现问题
            引用消息队列以后，可以将要发送短信这个任务作为一个消息放入到消息队列中，等待消息队列一条一条的处理消息就可以了
##### 通过消息队列就实现了一个异步解耦的过程，短信发送时，我们保证短信放到消息队列就可以了，然后就可以接着做发送短信后面的事情了，
##### 整个设计就简单了很多，不需要在下单成功后过多的考虑发送短信的问题，只要放到消息队列交给队列就可以了， 
##### 通过消息保证了最终的一致性，通过这样的设计保证短信一定发送给用户，即使当前短信发送有问题，我们也可以通过消息队列来保证在短信服务恢复后将短息发送出去，只是不是很及时
    如短信发送完之后，还要发送邮件， 有了消息队列，我们就不用同步的等待执行了，完全可以并行操作，可以更快的结束，减少甚至不出现并发现象
### 消息队列的特性
#### 1、业务无关：只做消息分发
#### 2、FIFO ：先投递先到达
#### 3、容灾 ：节点的动态增删和消息的持久化 
#### 4、性能：吞吐量提升，系统内部通信效率提高
### 为什么需要消息队列
#### [生产] 和 [消费]的速度或稳定性等因素不一致
##### 1、业务系统触发了发送短息的发送申请，但是短息发送的模块速度根不上，需要将来不及处理的消息暂存一下，缓存压力，就可以将短息的申请发送丢到消息队列里，消息发送模块可以慢慢的从消息队列里取出消息进行处理
##### 2、当前系统调用远程系统下订单，成本较高，且因为网络等因素不稳定，攒一批一起来发送
##### 3、任务处理系统，先把用户发起的任务请求接收过来存放在消息队列中，开启多个应用程序从队列中取出任务来处理
### 消息队列的好处
#### 业务解耦
##### 解耦：就是一个事务之关心核心的流程，而需要依赖其他系统但不那么重要的事情有通知即可，无需等待结果，所以基于消息的模式关心的是 -- 通知，而不是  处理
#### 最终一致性
##### 俩个系统的状态保持一致，要么都成功，要么都失败
#### 广播
#### 错峰与流控

### 消息队列举例
#### Kafka
#### RabbitMQ

## 高并发之应用拆分
### 应用拆分 (系统拆分)
#### 业务优先
#### 循序渐进
    拆分   测试， 
#### 兼顾技术 ： 重构、分层
#### 可靠测试
### 应用拆分 - 思考
#### 1、应用之间通信 ： RPC(dubbo等)、消息队列
#### 2、应用之间的数据库设计 ： 每个应用都有独立的数据库
#### 3、避免事务操作跨应用
### 应用拆分使用的组件
#### 消息队列
#### dubbo框架
##### dubbo 是一种分布式的服务框架， webService也是一种服务框架，但是webService不是分布式的服务框架
#### spring cloud 微服务 
##### 微服务是把一个大型的单个应用程序和服务，拆分成数个甚至数十个单独的微服务，它可扩展单个组件，而不是整个应用程序堆栈，从而满足服务等级协议，微服务是围绕业务领域组件来创建应用，这些应用可以独立的进行开发、管理和迭代，在分担的组件中使用云架构和平台式部署管理服务功能，使服务交互变得更加简单，它的本质是用一下功能比较明确、业务比较精炼的服务区解决更大更实际的问题。
##### 1、分布式管理
##### 2、按照业务划分而不是按照技术划分
##### 3、有生命的产品而不是项目
##### 4、强服务个体弱通信
##### 5、自动化运维
##### 6、具有高度的容错性
##### 7、可以快速的演化和迭代

## 高并发之数据库切库、分库、分表
### 数据库瓶颈
##### 单个库数据量太大（ 1T - 2T ） ： 多个库
##### 单个数据库服务器压力过大、读写瓶颈 ：多个库
##### 单个表数据量过大 ：分表
#### 数据库切库
##### 1、切库的基础及实际运用 ：读写分离
##### 2、自定义注解完成数据库切库 - 代码实现
###### 现在大型的电商项目在数据库层面都是采用读写分离技术，就是一个主库，多个从库，主库主要负责数据更新和实时数据查询，从库主要负责非实时数据的查询，因为在实际应用中，数据库都是读多写少的，而读取数据通常耗时比较长，占用数据库服务器CPU比较多，影响用户体验，通常做法就是把查询从主库抽离出来，采用多个从库，使用负载均衡，减轻每个从库的查询压力，
###### 用读写分离技术的目标：是可以有效减轻主库的压力，又可以把用户查询数据的请求分发到不同的从库上，运行时把数据源动态的植入到程序中，让程序制定的程序来选择操作主库还是操作从库，这里主要使用的技术是注解SpringAOP
##### 3、支持多数据源、分库
###### 支持多数据源和分库的区别是
##### 4、什么时候考虑分表？
###### 当一个表的数据量很大， 大到当我们做了SQL和索引优化之后还是慢，我们就必须考虑分表了，
##### 5、横向(水平)分表 与 纵向(垂直)分表
###### 横向切割为同样结构的不同的表，根据记录id和表的个数进行取模来划分，保证每个表的容量不会太大，保证单表的查询效率
###### 纵向本表，在一个表里的内容划分成多个表，这个时候一般是根据数据的活跃度来划分
##### 6、数据库分表 ： mybatis分表插件 shardbatis2.0

## 高并发之高可用的一些手段
### 高可用的一些手段
#### 1、任务调度系统分布式：elastic-job + zookeeper
#### 2、主备切换： apache curator + zookeeper 分布式锁实现
###### 两台服务器启动时，他们都去尝试对zookeeper的一个相同的节点进行加锁，两台服务器中只会有一台服务器可以持有这把锁，没有拿到锁的服务器一直尝试去获取锁，拿到锁的服务器它尝试尽可能长的时间去锁入这个节点，直到重启或者不能持有这把锁时，就把锁释放了，一旦锁释放了，参与竞争的另一台服务器就会立马持有这把锁，持有锁后开始对外提供服务，没有拿到锁的服务器继续尝试获取锁。
#### 3、监控报警机制
###### 